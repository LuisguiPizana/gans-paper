GANS Parameter Testing:
2024-08-31-4:
{Test conditional gan architecture with Exponential Decay Learning Rate Schedule. Compared to 2024-08-31-5, they are the same model but with a different LR Schedule, test 5 uses a step decay learning rate. I think its better than the next version (with the old schedule). It seems to be more stable over time and can address details later on the training process. Although IS score and FID score seem very similar.
}

2024-08-31-5:
{Test conditional gan architecture with Step Decay Learning Rate Schedule.
}

2024-08-31-BAtchNorm_A_paperLR:
{Test conditional gan architecture with Exponential Decay Learning Rate Schedule. The initial LR was set to 0.1 as specified in the paper. This model has a huge instability in training probably derived from the use of batch normalization and a high learning rate. 
Now I believe that the ADAM optimizer is the one that is not suited for working with larger learning rates. I need to check this.
}

2024-09-02-01_reduced_dropout:
{
Testing same model as 2024-08-31-4 but changing the dropout rate from 0.5 to 0.1.
Compared to the training of 2024-08-31-4, we can see that the disc fake loss decreases to 0 by the end of epoch 2 as well as the real loss converging to 0.4.
At this point the gradient norm decreases to almost 0 for the generator and the generator error increases significantly. 
It is definitely a significant change that significantly deteriorates performance. 
}

2024-09-02-02:
{
Testing same model as 2024-08-31-4 but changing the initial LR to 0.1 from 0.2 and the img_size of the generator from 200 to 400.
Performance is slightly worse than the 2024-08-31-4 model in FID and IS. We can also see deterioration in image sampling. Gradients are more unstable and larger. 
}

2024-09-02-03:
{
Testing same model as 2024-08-31-4 but changing just the initial LR to 0.1 from 0.2. 
Samples look very similar to the previous test. However it is slightly better in IS.
}

2024-09-03-01:
{
Alecsa Godric uses a normalization 0, 1 for the input data and a tanh function as the output of the generator.
The main change in this model is the data processing. Changing from [-1 - 1] to [0-1] and the usage of sigmoid activation function in the generator instead of tanh to match this new range.
Also we are using starting lr of 0.0001.
It seems that the discriminator is learning extremely fast leading to a poor learning of the generator. 
}

2024-09-03-02:
{
Same as before but using starting lr of 0.0002 instead of 0.0001.
It seems that the discriminator is learning extremely fast leading to a poor learning of the generator. 
}

2024-09-03-03:
{
Using lr of 0.0002 and removing the batch normalization from the generator. The goal is to see if this can accelerate the learning of the generator. 
}

2024-09-03-04:
{
Using lr of 0.0002 for generator and 0.00002 for the discriminator. We are using again batch normalization.
}

2024-09-03-05:
{
Using lr of 0.0002 for generator and 0.00009 for the discriminator. We are using again batch normalization.
}

2024-09-04-01:
{
Using lr of 0.0002 for generator and discriminator. This test is with the tanh activation function again. I will try removing the LR Schedule and let Adam do the gradient adjustments.
}

2024-09-04-02:
{
Same as 2024-09-04-01 but now I will try to set the true label to 1 instead of 0.85.
}

2024-09-04-03:
{
Testing aleksa godric architecture. Note that discriminator has no batch normalization, there is no dropout and generator has one extra layer. Discriminator is based also on fully connected dense layers.
This model generates better results, oscillating around 2.75 IS score and reaching 145 FID Score. In comparison to other models around 2.6 and 300 respectively. 
}

2024-09-05-01:
{
This model is based on this architecture 2024-09-04-02 with several changes:
- Changing real label from 0.1 to 1.0 (this was a mistake in reference model)
- Add Batch Normalization to Generator
}


2024-09-05-02:
{
This model is based on this architecture 2024-09-05-01 with several changes:
- Changing dropout to 0.3
- Reducing the number of units in Maxout Pieces.
- Adding a 0.2 slope in LeakyReLU activations.
The main address hypothesis is that we might have a very large model for the amount of data we have.
}


2024-09-05-03:
{
This model is based on this architecture 2024-09-05-02 with several changes:
- Removing dropout from joint layers.
- Reducing the number of units in Maxout Units and pieces.
The main address hypothesis is that we might have a very large model for the amount of data we have. Removing dropout tries to balance the reduction in model size.
}

Notes:
Training 2024-09-05-03  and 2024-09-05-01 have a significantly worse performance than 2024-09-05-02 which is a very good model.The model receives its improvement from changing the dropout to 0.3 from 0.5, reducing the number of units in the Maxout Pieces and adding a 0.2 slope in LeakyReLU activations. 

I want to test where the most improvement comes from. The following experiments will be exploring individual changes taking 2024-09-05-01 as a starting model.


On the other hand I believe the reduction of dropout and reducing the number of maxout units might have had significant impact. We could try 


2024-09-06-01:
{
This model copies 2024-09-05-02, restores the dropout in maxout pieces and adding back the maxout units. 
It also reduces the maxout pieces compared to 2024-09-05-02.
}

2024-09-06-02:
{
This experiment uses as base 2024-09-06-01 and reduces the dropout rate from 0.3 to 0.2. 
}

2024-09-06-03:
{
This experiment tries to isolate the effect of the reduction of the dropout rate. It adds the decreased maxout pieces to the architecture.
}

All the recent experiments have acceptable performance. The best ones are 2024-09-05-02 (the base from previous batch) and 2024-09-06-03. So the reduction of units did not result favorable (2024-09-06-01) while the reduction in dropout only has some benefits and disadvantages. A notable change is the intensity of numbers with less dropout but they seem to be a little bit messier and less clear. 

I will try to increase the number of units in the pieces with a 0.2 and 0.3 dropout rate. I will also try to enhance the generator adding an extra layer.

2024-09-07-01:
{
This experiment incrases the number of units in the discriminator from 2024-09-06-03 around 12%.
}

2024-09-07-02:
{
This experiment incrases the number of units in the discriminator from 2024-09-06-03 around 12% and uses the dropout rate from 2024-09-05-02 which is 0.3.
}


2024-09-07-03:
{
This experiment incrases the number of units in the discriminator from 2024-09-06-03 around 12% and uses the dropout rate from 2024-09-05-02 which is 0.3. It also adds an extra layer in the generator making a slower size increase.
}

2024-09-07-04:
{
This layer uses 2024-09-05-02 as base and adds the extra layer in the generator. 
}


2024-09-07-05:
{
This architecture decreases the dropout rate in the generator and discriminator. Apart from this we are making changes by increasing the first joint layer of the generator while decreasing the label layer. Also decreasing the units in the discriminator around 12%
}

2024-09-07-06:
{
This architecture parts from the 2024-09-05-02 and reduces the size of the discriminator units and the dropout to 0.1 from 0.3.
Reducing that much the dropout is not giving good results.
}

2024-09-07-07:
{
Increasing the dropout back to 0.2. I am using the paper architecture for the generator and I am decreasing further the number of neurons in the discriminator.
}

2024-09-07-08:
{
Same as 2024-09-07-07. Increasing the dropout back to 0.2. I am using the paper architecture for the generator and I am decreasing further the number of neurons in the discriminator.
Also changing the generator to have 512 joint neurons instead of 200 and reducing the text embedding to 512 from 1000.
}

We can see that the change in architecture from having a text embedding layer of 1000 neurons to reducing this to 512 and incrasing the first joint layer from 200 to 512 has a slight decrease in the FID score. The FID score pays attention to the distribution and reducing the label layer could have an impact, also the performance IS score is being similar to the previous architecture.

We can also look at the impacts of the decrease in the dropout rate. From the model 024-09-05-02 to the next batch which tests decreasing units and dropout we can see that the clearness of numbers is not matched. Intensity might improve in several cases but they tend to get noiser. 

Now we will try to raise the dropout rate back to 0.3 and test the new smaller models. We are hoping to mantain the dropout clarity but increasing the intensity by reducing the number of units.

2024-09-08-01:
{
This architecture has a generator of 512 and 512 neurons and the diminished unit sizes. We will try to raise the dropout to see the change in performance. This is because 2024-09-05-02 maintains clean numbers, which is hard for other models with the lower rates we have tried. Increase to 0.3 everywhere and the LeakyRelu Slope to 0.3
}

2024-09-08-02:
{
Same as 2024-09-08-01, but testing an increase in the generator label layer from 512 back to 1000.
}

From the two above models I believe the one with 512 neurons in the label layer of the generator is slightly better. It achieves results comparable with 2024-09-05-02. In the next models I will start with 2024-09-08-01 and test a slight decrease in the generator final layer and decrease the LeakyReLU slope.I will try also decreasing the label segment from 512 to 256.


2024-09-09-01:
{
This architecture is based on the 2024-09-08-01 architecture with small changes. I will be testing the reduction of the joint layer from 1200 to 1024 neurons in the generator and a decrease in the LeakyRelu slope from .3 to .2.
}


2024-09-09-02:
{
With the same changes as 2024-09-09-01, I will be testing a reduction in the label layer of the generator from 512 to 256 neurons.
}

From the previous two tests, it seems that the 2024-09-09-02 experiment has better results. It almost reaches 200 FID score (which is about 100 points less than 2024-09-09-01) and around the same as 2024-09-09-01 which is around 2.7 IS score. Visually, the images are more clear but we can see there are some loose pixels in them. What I mean by this is some white pixels in the dark side (where there is supposed to be no number).


For the next experiments I will try to address the changes suggested by chatGPT 01.
2024-30-09-02:
{
This experiment addresses thae first 3 changes which are reducing the label_hidden_units rom 256 to 128 in the generator. I will also reduce the leaky_relu slope from 0.2 to 0.1 to try to address the "loose pixels" problem. Finally I will try to reduce the Discriminator's Learning Rate to 0.0001 from 0.0002.
}


2024-30-09-02:
{
Like before but increasing the maxout units in the joint layer from 3 to 4 and in the img layer from 4 to 5 (changes in Discriminator).
}
